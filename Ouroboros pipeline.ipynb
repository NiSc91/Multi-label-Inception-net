{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget to set the args marked with ########. With current params first block downloads all 60gb+ of train images (not randomly if _max is set larger than dataset)!\n",
    "Make sure images/ is empty and the resize outdir does not exist when running from the first block\n",
    "Viktor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1014544/1014544 [01:26<00:00, 11678.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import urllib3\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from urllib3.util import Retry\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "#Chooses randomly if _max (the number of images to get) is smaller than the dataset\n",
    "\n",
    "#Fixing the args\n",
    "#example args\n",
    "#downloader.py test.json test\n",
    "fix_args=['whatever','train.json','alltrain']   ################################\n",
    "\n",
    "def download_image(fnames_and_urls):\n",
    "    \"\"\"\n",
    "    download image and save its with 90% quality as JPG format\n",
    "    skip image downloading if image already exists at given path\n",
    "    :param fnames_and_urls: tuple containing absolute path and url of image\n",
    "    \"\"\"\n",
    "    fname, url = fnames_and_urls\n",
    "    if not os.path.exists(fname):\n",
    "        http = urllib3.PoolManager(retries=Retry(connect=3, read=2, redirect=3))\n",
    "        response = http.request(\"GET\", url)\n",
    "        image = Image.open(io.BytesIO(response.data))\n",
    "        image_rgb = image.convert(\"RGB\")\n",
    "        image_rgb.save(fname, format='JPEG', quality=90)\n",
    "\n",
    "\n",
    "def parse_dataset(_dataset, _outdir, _max=2000000):     ################################\n",
    "    \"\"\"\n",
    "    parse the dataset to create a list of tuple containing absolute path and url of image\n",
    "    :param _dataset: dataset to parse\n",
    "    :param _outdir: output directory where data will be saved\n",
    "    :param _max: maximum images to download (change to download all dataset)\n",
    "    :return: list of tuple containing absolute path and url of image\n",
    "    \"\"\"\n",
    "    _fnames_urls = []\n",
    "    with open(dataset, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        for image in data[\"images\"]:\n",
    "            url = image[\"url\"]\n",
    "            fname = os.path.join(outdir, \"{}.jpg\".format(image[\"imageId\"]))\n",
    "            _fnames_urls.append((fname, url))\n",
    "            \n",
    "        #randomize\n",
    "        random_choice = []\n",
    "        if _max < len(_fnames_urls):\n",
    "            random_choice = np.random.choice(len(_fnames_urls), _max, replace=False).tolist()\n",
    "            _fnames_urls = [_fnames_urls[i] for i in random_choice]\n",
    "        \n",
    "    return _fnames_urls, random_choice\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if len(fix_args) != 3:\n",
    "        print(\"error: not enough arguments\")\n",
    "        #sys.exit(0)\n",
    "\n",
    "    # get args and create output directory\n",
    "    dataset, outdir = fix_args[1:]\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "        print('made output dir called '+outdir)\n",
    "\n",
    "    # parse json dataset file\n",
    "    fnames_urls, random_choice = parse_dataset(dataset, outdir)\n",
    "    print(len(random_choice))\n",
    "    #ranc = open('random_choice.txt', 'w')\n",
    "    #for item in random_choice:\n",
    "        #ranc.write(\"%s\\n\" % item)\n",
    "    with open('random_choice.csv', 'w') as myfile:\n",
    "        wr = csv.writer(myfile)\n",
    "        wr.writerow(random_choice)\n",
    "    \n",
    "    # download data\n",
    "    pool = multiprocessing.Pool(processes=12)\n",
    "    with tqdm(total=len(fnames_urls)) as progress_bar:\n",
    "        for _ in pool.imap_unordered(download_image, fnames_urls):\n",
    "            progress_bar.update(1)\n",
    "\n",
    "    #sys.exit(1)\n",
    "    print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.7.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import os, sys\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "#rescale and pad with white (3 minutes for 10k 256x256)\n",
    "# combined https://jdhao.github.io/2017/11/06/resize-image-to-square-with-padding/ + https://gist.github.com/ihercowitz/642650/f01986c0b1ebd04be588b196eb3ffefe9853e113\n",
    "\n",
    "def resizeImage(infile, output_dir=\"scaled\", desired_size = 299):\n",
    "     outfile = os.path.splitext(infile)[0]\n",
    "     extension = os.path.splitext(infile)[1]\n",
    "\n",
    "     #if (cmp(extension, \".jpg\")):\n",
    "     if extension != \".jpg\":\n",
    "        return\n",
    "\n",
    "     if infile != outfile:\n",
    "        im = Image.open(dir+infile)\n",
    "        old_size = im.size  # old_size[0] is in (width, height) format\n",
    "        ratio = float(desired_size)/max(old_size)\n",
    "        new_size = tuple([int(x*ratio) for x in old_size])\n",
    "        im = im.resize(new_size, Image.ANTIALIAS)\n",
    "        new_im = Image.new(\"RGB\", (desired_size, desired_size),color=(255,255,255))\n",
    "        new_im.paste(im, ((desired_size-new_size[0])//2,(desired_size-new_size[1])//2))\n",
    "        new_im.save(output_dir+outfile+extension,\"JPEG\")\n",
    "        #except IOError:\n",
    "            #print(\"cannot reduce image for \", infile)\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    dir = 'alltrain/'    ################################\n",
    "    desired_size = 299\n",
    "    output_dir = \"images/alltrain299/\"    ################################\n",
    "    #dir = os.getcwd()\n",
    "    \n",
    "\n",
    "    if not os.path.exists(os.path.join(dir,output_dir)):    #######\n",
    "        os.mkdir(output_dir)\n",
    "\n",
    "    for file in os.listdir(dir):\n",
    "        resizeImage(file,output_dir,desired_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check pipelog.txt to see whether retrain.py is actually working!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nohup python retrain.py --bottleneck_dir=bottlenecks --how_many_training_steps 500 --model_dir=model_dir --output_graph=retrained_graph.pb --output_labels=retrained_labels.txt --summaries_dir=retrain_logs --image_dir=images > pipelog.txt 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!nohup python3 retrain.py --bottleneck_dir=bottlenecks --how_many_training_steps 500 --model_dir=model_dir --output_graph=retrained_graph.pb --output_labels=retrained_labels.txt --summaries_dir=retrain_logs --image_dir=images > pipelog.txt 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from PIL import Image, ImageOps\n",
    "import os, sys\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "#1 Predict probabilities for labels, save .csv\n",
    "\n",
    "#Runtime 109s for 100 (155kB, with .4 float precision, on Intel i7), 1445s for 1000 imgs (1.6MB)\n",
    "\n",
    "#params\n",
    "input_path = 'test/'\n",
    "first = 1\n",
    "last = 39706     ################################\n",
    "image_numbers = range(first, last+1)\n",
    "save_path = 'predictions/'\n",
    "save_as = 'ffall0.csv'     ################################\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#modified from https://github.com/BartyzalRadek/Multi-label-Inception-net\n",
    "# Loads label file, strips off carriage return\n",
    "label_lines = [line.rstrip() for line \n",
    "                   in tf.gfile.GFile(\"labels.txt\")]\n",
    "top_k = np.asarray([int(lbl) for lbl in label_lines])\n",
    "\n",
    "all_predictions = np.empty([len(image_numbers),len(label_lines)+1])\n",
    "\n",
    "# Unpersists graph from file\n",
    "with tf.gfile.FastGFile(\"retrained_graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Feed the image_data as input to the graph and get first prediction\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')    \n",
    "    \n",
    "    #iterate over images\n",
    "    counter = 0\n",
    "    for i, image_num in enumerate(image_numbers):\n",
    "        image_path = input_path+str(image_num)+'.jpg'\n",
    "        #print(image_path)\n",
    "        # Read in the image_data\n",
    "        image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "\n",
    "\n",
    "        predictions = sess.run(softmax_tensor, \\\n",
    "                 {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        all_predictions[i] = np.insert(predictions, 0, image_num)\n",
    "\n",
    "        if (counter == 5000):\n",
    "            print(image_num)\n",
    "            np.savetxt(save_path+save_as, all_predictions, delimiter=\",\", fmt='%1.4f')\n",
    "            counter = 0\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "np.savetxt(save_path+save_as, all_predictions, delimiter=\",\", fmt='%1.4f')\n",
    "\n",
    "#header\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print('Saved to '+save_path+save_as)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = 'validation/'\n",
    "first = 1\n",
    "last = 9897     ################################\n",
    "image_numbers = range(first, last+1)\n",
    "save_path = 'predictions/'\n",
    "save_as = 'ffall0val.csv'     ################################\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#modified from https://github.com/BartyzalRadek/Multi-label-Inception-net\n",
    "# Loads label file, strips off carriage return\n",
    "label_lines = [line.rstrip() for line \n",
    "                   in tf.gfile.GFile(\"labels.txt\")]\n",
    "top_k = np.asarray([int(lbl) for lbl in label_lines])\n",
    "\n",
    "all_predictions = np.empty([len(image_numbers),len(label_lines)+1])\n",
    "\n",
    "# Unpersists graph from file\n",
    "with tf.gfile.FastGFile(\"retrained_graph.pb\", 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Feed the image_data as input to the graph and get first prediction\n",
    "    softmax_tensor = sess.graph.get_tensor_by_name('final_result:0')    \n",
    "    \n",
    "    #iterate over images\n",
    "    counter = 0\n",
    "    for i, image_num in enumerate(image_numbers):\n",
    "        image_path = input_path+str(image_num)+'.jpg'\n",
    "        #print(image_path)\n",
    "        # Read in the image_data\n",
    "        image_data = tf.gfile.FastGFile(image_path, 'rb').read()\n",
    "\n",
    "\n",
    "\n",
    "        predictions = sess.run(softmax_tensor, \\\n",
    "                 {'DecodeJpeg/contents:0': image_data})\n",
    "\n",
    "        all_predictions[i] = np.insert(predictions, 0, image_num)\n",
    "\n",
    "        if (counter == 5000):\n",
    "            print(image_num)\n",
    "            np.savetxt(save_path+save_as, all_predictions, delimiter=\",\", fmt='%1.4f')\n",
    "            counter = 0\n",
    "\n",
    "        counter += 1\n",
    "\n",
    "np.savetxt(save_path+save_as, all_predictions, delimiter=\",\", fmt='%1.4f')\n",
    "\n",
    "#header\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "print('Saved to '+save_path+save_as)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
